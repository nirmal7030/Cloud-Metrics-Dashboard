name: Test & Deploy Cloud Metrics Dashboard to AWS EC2

on:
  push:
    branches: ["main"]

env:
  AWS_REGION: ${{ secrets.AWS_REGION }}
  ECR_REPOSITORY: cloud-metrics-dashboard
  IMAGE_TAG: latest

jobs:
  test:
    name: Stage 1 - Build & Test
    runs-on: ubuntu-latest

    # âœ… expose durations so deploy job can use them
    outputs:
      build_duration: ${{ steps.build_step.outputs.build_duration }}
      test_duration:  ${{ steps.test_step.outputs.test_duration }}

    steps:
      - name: Checkout source
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      # â±ï¸ Timed install (build step)
      - name: Install dependencies (timed)
        id: build_step
        run: |
          set -e
          START=$(date +%s)
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          END=$(date +%s)
          echo "build_duration=$((END-START))" >> "$GITHUB_OUTPUT"

      # â±ï¸ Timed tests
      - name: Run Django tests (timed)
        id: test_step
        run: |
          set -e
          START=$(date +%s)
          python manage.py test
          END=$(date +%s)
          echo "test_duration=$((END-START))" >> "$GITHUB_OUTPUT"

  deploy:
    name: Stage 2 - Deploy to EC2
    runs-on: ubuntu-latest
    needs: test  # âœ… deploy runs only if test succeeds

    steps:
      - name: Checkout source
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Build, tag, and push Docker image to ECR
        run: |
          docker build -t $ECR_REPOSITORY:$IMAGE_TAG .
          docker tag $ECR_REPOSITORY:$IMAGE_TAG ${{ steps.login-ecr.outputs.registry }}/$ECR_REPOSITORY:$IMAGE_TAG
          docker push ${{ steps.login-ecr.outputs.registry }}/$ECR_REPOSITORY:$IMAGE_TAG

      - name: Deploy on EC2
        uses: appleboy/ssh-action@v0.1.7
        with:
          host: ${{ secrets.EC2_HOST }}
          username: ubuntu
          key: ${{ secrets.EC2_SSH_KEY }}
          script: |
            set -e

            REGISTRY="${{ steps.login-ecr.outputs.registry }}"
            IMAGE="$REGISTRY/${{ env.ECR_REPOSITORY }}:${{ env.IMAGE_TAG }}"

            # Stop & remove old container if it exists
            sudo docker stop metrics-bench || true
            sudo docker rm metrics-bench || true

            # ðŸ”¥ Clean up unused Docker data so we don't run out of space
            sudo docker system prune -af --volumes || true

            # Login to ECR
            aws ecr get-login-password --region ${{ env.AWS_REGION }} \
              | sudo docker login --username AWS --password-stdin "$REGISTRY"

            # Pull latest image and run container, passing BENCH_API_KEY into Django
            sudo docker pull "$IMAGE"
            sudo docker run -d -p 80:8000 --name metrics-bench \
              -e BENCH_API_KEY=${{ secrets.BENCH_API_KEY }} \
              "$IMAGE"

      # ðŸŒ Post live novel metrics to your Django API
      - name: Post novel metrics to app
        env:
          # direct URL of your EC2 instance
          APP_BASE_URL:  http://ec2-98-80-122-141.compute-1.amazonaws.com
          BENCH_API_KEY: ${{ secrets.BENCH_API_KEY }}
          BUILD_DUR:     ${{ needs.test.outputs.build_duration }}
          TEST_DUR:      ${{ needs.test.outputs.test_duration }}
        run: |
          set -e
          BUILD_DUR=${BUILD_DUR:-0}
          TEST_DUR=${TEST_DUR:-0}

          echo "Build duration: $BUILD_DUR seconds"
          echo "Test duration:  $TEST_DUR seconds"

          # --- Derive your novel metrics (simple heuristic) ---

          # LCE (Layer Cache Efficiency) - higher if build is faster
          LCE=$(( 100 - BUILD_DUR / 2 ))
          if [ "$LCE" -lt 0 ]; then LCE=0; fi
          if [ "$LCE" -gt 100 ]; then LCE=100; fi

          # PRT (Pipeline Recovery Time) - approximate with test duration for now
          PRT=$TEST_DUR

          # SMO (Secrets Management Overhead) - constant placeholder
          SMO=1

          # DEPT (Dynamic Env Provisioning Time) - tie to test duration for now
          DEPT=$TEST_DUR

          # CLBC (Cross-Layer Build Consistency) - 100 for successful pipeline
          CLBC=100

          # Optional overall score as simple average of LCE & CLBC
          VALUE=$(( (LCE + CLBC) / 2 ))

          cat > payload.json <<EOF
          {
            "source": "github",
            "workflow": "${{ github.workflow }}",
            "run_id": "${{ github.run_id }}",
            "run_attempt": "${{ github.run_attempt }}",
            "branch": "${{ github.ref_name }}",
            "commit_sha": "${{ github.sha }}",

            "lce":  $LCE,
            "prt":  $PRT,
            "smo":  $SMO,
            "dept": $DEPT,
            "clbc": $CLBC,

            "value": $VALUE,
            "notes": "auto-ingest of novel metrics from CI"
          }
          EOF

          echo "Posting payload to $APP_BASE_URL/api/metrics/ingest/ ..."
          cat payload.json

          curl -v -X POST "$APP_BASE_URL/api/metrics/ingest/" \
            -H "Content-Type: application/json" \
            -H "X-Bench-Key: $BENCH_API_KEY" \
            --data @payload.json
